{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(\"/home/me/workspace/det_remota/trabalho_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report as DataFrame:\n",
      "              precision  recall  f1-score  support\n",
      "Class A            0.82    0.77      0.79     65.0\n",
      "Class B            0.70    0.67      0.68     60.0\n",
      "Class C            0.73    0.82      0.77     55.0\n",
      "macro avg          0.75    0.75      0.75    180.0\n",
      "weighted avg       0.75    0.75      0.75    180.0\n",
      "\n",
      "Accuracy: 0.75 (135/180)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union, List, Optional, Dict\n",
    "\n",
    "\n",
    "def classification_report_from_confusion_matrix(\n",
    "    confusion_matrix: pd.DataFrame,\n",
    "    labels: Optional[List[Union[str, int]]] = None,\n",
    "    digits: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute classification metrics from a confusion matrix and return a DataFrame with the results.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    confusion_matrix : pd.DataFrame\n",
    "        A pandas DataFrame representing the confusion matrix.\n",
    "        Rows should represent actual classes and columns should represent predicted classes.\n",
    "        \n",
    "    labels : List[Union[str, int]], optional\n",
    "        List of class labels. If None, will use the column names of the confusion matrix.\n",
    "        \n",
    "    digits : int, default=2\n",
    "        Number of decimal places to round to in the output.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the classification report with precision, recall, f1-score, and support for each class.\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        # Use column names if no labels are provided\n",
    "        if isinstance(confusion_matrix.columns, pd.Index):\n",
    "            labels = confusion_matrix.columns.tolist()\n",
    "        else:\n",
    "            labels = list(range(len(confusion_matrix.columns)))\n",
    "    \n",
    "    # Ensure confusion matrix is numpy array for calculations\n",
    "    if isinstance(confusion_matrix, pd.DataFrame):\n",
    "        cm = confusion_matrix.values\n",
    "    else:\n",
    "        cm = np.array(confusion_matrix)\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    n_classes = cm.shape[0]\n",
    "    class_metrics = {}\n",
    "    \n",
    "    # Total samples\n",
    "    total_samples = np.sum(cm)\n",
    "    \n",
    "    # Calculate global accuracy\n",
    "    accuracy = np.trace(cm) / total_samples\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        # True positives: diagonal element for this class\n",
    "        tp = cm[i, i]\n",
    "        \n",
    "        # False positives: sum of column i (predicted as class i) minus true positives\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        \n",
    "        # False negatives: sum of row i (actual class i) minus true positives\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        \n",
    "        # True negatives: sum of all elements except those in row i or column i, plus tp\n",
    "        tn = total_samples - (tp + fp + fn)\n",
    "        \n",
    "        # Support: number of actual samples in this class\n",
    "        support = np.sum(cm[i, :])\n",
    "        \n",
    "        # Calculate metrics (handle division by zero)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        class_metrics[labels[i]] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1-score': f1,\n",
    "            'support': support\n",
    "        }\n",
    "    \n",
    "    # Calculate macro averages\n",
    "    macro_precision = np.mean([class_metrics[label]['precision'] for label in labels])\n",
    "    macro_recall = np.mean([class_metrics[label]['recall'] for label in labels])\n",
    "    macro_f1 = np.mean([class_metrics[label]['f1-score'] for label in labels])\n",
    "    \n",
    "    # Calculate weighted averages\n",
    "    total_support = sum(class_metrics[label]['support'] for label in labels)\n",
    "    weighted_precision = sum(class_metrics[label]['precision'] * class_metrics[label]['support'] \n",
    "                            for label in labels) / total_support\n",
    "    weighted_recall = sum(class_metrics[label]['recall'] * class_metrics[label]['support'] \n",
    "                         for label in labels) / total_support\n",
    "    weighted_f1 = sum(class_metrics[label]['f1-score'] * class_metrics[label]['support'] \n",
    "                     for label in labels) / total_support\n",
    "    \n",
    "    # Create the DataFrame for the classification report\n",
    "    report_data = {}\n",
    "    \n",
    "    # Add metrics for each class\n",
    "    for label in labels:\n",
    "        report_data[label] = {\n",
    "            'precision': round(class_metrics[label]['precision'], digits),\n",
    "            'recall': round(class_metrics[label]['recall'], digits),\n",
    "            'f1-score': round(class_metrics[label]['f1-score'], digits),\n",
    "            'support': int(class_metrics[label]['support'])\n",
    "        }\n",
    "    \n",
    "    # Add macro average\n",
    "    report_data['macro avg'] = {\n",
    "        'precision': round(macro_precision, digits),\n",
    "        'recall': round(macro_recall, digits),\n",
    "        'f1-score': round(macro_f1, digits),\n",
    "        'support': total_support\n",
    "    }\n",
    "    \n",
    "    # Add weighted average\n",
    "    report_data['weighted avg'] = {\n",
    "        'precision': round(weighted_precision, digits),\n",
    "        'recall': round(weighted_recall, digits),\n",
    "        'f1-score': round(weighted_f1, digits),\n",
    "        'support': total_support\n",
    "    }\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    report_df = pd.DataFrame(report_data).T\n",
    "    \n",
    "    # Add accuracy as a separate attribute\n",
    "    report_df.attrs['accuracy'] = round(accuracy, digits)\n",
    "    report_df.attrs['accuracy_count'] = f\"{int(np.trace(cm))}/{total_samples}\"\n",
    "    \n",
    "    return report_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example confusion matrix as a pandas DataFrame\n",
    "    # Rows are true labels, columns are predicted labels\n",
    "    cm = pd.DataFrame([\n",
    "        [50, 10, 5],\n",
    "        [8, 40, 12],\n",
    "        [3, 7, 45]\n",
    "    ], index=['Class A', 'Class B', 'Class C'], \n",
    "       columns=['Class A', 'Class B', 'Class C'])\n",
    "    \n",
    "    # Call the function and get the report DataFrame\n",
    "    report_df = classification_report_from_confusion_matrix(cm)\n",
    "    \n",
    "    # Display the report DataFrame\n",
    "    print(\"Classification Report as DataFrame:\")\n",
    "    print(report_df)\n",
    "    \n",
    "    # Display accuracy information from the DataFrame attributes\n",
    "    print(f\"\\nAccuracy: {report_df.attrs['accuracy']} ({report_df.attrs['accuracy_count']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class A</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.79</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class B</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class C</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score  support\n",
       "Class A            0.82    0.77      0.79     65.0\n",
       "Class B            0.70    0.67      0.68     60.0\n",
       "Class C            0.73    0.82      0.77     55.0\n",
       "macro avg          0.75    0.75      0.75    180.0\n",
       "weighted avg       0.75    0.75      0.75    180.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                  |   aquaculture |   river_lake_ocean |   forest |   farming |   non_vegetated |\n",
      "|:-----------------|--------------:|-------------------:|---------:|----------:|----------------:|\n",
      "| aquaculture      |          5514 |                339 |        3 |         0 |            8429 |\n",
      "| river_lake_ocean |           551 |              13445 |        0 |         0 |              47 |\n",
      "| forest           |             0 |                 14 |    59347 |        85 |            1116 |\n",
      "| farming          |             0 |                  3 |      937 |     30693 |           29245 |\n",
      "| non_vegetated    |           143 |                110 |      272 |       273 |          203504 |\n"
     ]
    }
   ],
   "source": [
    "CONF_MATRIX_PATH = Path(\"data/qgis_outputs/dzetsaka_models\")\n",
    "\n",
    "conf_matrix_dict = {}\n",
    "\n",
    "IDX_CLASS_MAPPING = {\n",
    "    0: 'aquaculture',\n",
    "    1: 'river_lake_ocean',\n",
    "    2: 'forest', \n",
    "    3: 'farming',\n",
    "    4: 'non_vegetated'\n",
    "}\n",
    "\n",
    "for conf_matrix_file in CONF_MATRIX_PATH.glob('*.conf_matrix'):\n",
    "    model_name = str(conf_matrix_file).split('/')[-1].replace('.conf_matrix', '')\n",
    "    conf_matrix_dict[model_name] = pd.read_csv(\n",
    "        conf_matrix_file,\n",
    "        skiprows=1, \n",
    "        header=None\n",
    "    )\n",
    "    conf_matrix_dict[model_name].rename(\n",
    "        columns=IDX_CLASS_MAPPING,\n",
    "        index=IDX_CLASS_MAPPING, \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "print(conf_matrix_dict['gaussian'].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Classification Report for model gaussian:\n",
      "                  precision  recall  f1-score   support\n",
      "aquaculture            0.89    0.39      0.54   14285.0\n",
      "river_lake_ocean       0.97    0.96      0.96   14043.0\n",
      "forest                 0.98    0.98      0.98   60562.0\n",
      "farming                0.99    0.50      0.67   60878.0\n",
      "non_vegetated          0.84    1.00      0.91  204302.0\n",
      "macro avg              0.93    0.76      0.81  354070.0\n",
      "weighted avg           0.90    0.88      0.87  354070.0\n",
      "\n",
      "Accuracy: 0.88 (312503/354070)\n",
      "\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, conf_mat_df in conf_matrix_dict.items():\n",
    "    # Call the function and get the report DataFrame\n",
    "    report_df = classification_report_from_confusion_matrix(conf_mat_df)\n",
    "    \n",
    "    print(\"\\n---------------------------------------------------\\n\")\n",
    "    # Display the report DataFrame\n",
    "    print(f\"Classification Report for model {model_name}:\")\n",
    "    print(report_df)\n",
    "    \n",
    "    # Display accuracy information from the DataFrame attributes\n",
    "    print(f\"\\nAccuracy: {report_df.attrs['accuracy']} ({report_df.attrs['accuracy_count']})\")\n",
    "    print(\"\\n---------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
